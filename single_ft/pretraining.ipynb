{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 다운 \n",
    "\n",
    "import kagglehub  #pip install kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"shubhammaindola/harry-potter-books\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 텍스트 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_/home/moon/LLM_Study/data/harry-potter-books/versions/1/02 Harry Potter and the Chamber of Secrets.txt 488771 characters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(filename):\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as file:\n",
    "        book_text = file.read()\n",
    "    \n",
    "    cleaned_text = re.sub(r'\\n+', ' ', book_text) # 줄바꿈을 빈칸으로 변경\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text) # 여러 빈칸을 하나의 빈칸으로\n",
    "    \n",
    "    print(\"cleaned_\" + filename, len(cleaned_text), \"characters\") # 글자 수 출력\n",
    "    \n",
    "    \n",
    "    with open( filename+\"_cleaned\", 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_text)\n",
    "    \n",
    "filenames_list = [\"/home/moon/LLM_Study/data/harry-potter-books/versions/1/02 Harry Potter and the Chamber of Secrets.txt\"]\n",
    "\n",
    "for filename in filenames_list:\n",
    "    clean_text(filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "글자수: 26 토큰수 6\n",
      "[18308, 14179, 373, 257, 18731, 13]\n",
      "Harry Potter was a wizard.\n",
      "18308\t -> Harry\n",
      "14179\t ->  Potter\n",
      "373\t ->  was\n",
      "257\t ->  a\n",
      "18731\t ->  wizard\n",
      "13\t -> .\n"
     ]
    }
   ],
   "source": [
    "import tiktoken #pip install tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "\n",
    "text = \"Harry Potter was a wizard.\"\n",
    "\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "print(\"글자수:\", len(text), \"토큰수\",len(tokens))\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "print(tokenizer.decode(tokens))\n",
    "\n",
    "for t in tokens:\n",
    "    print(f\"{t}\\t -> {tokenizer.decode([t])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ H ] -> [39] \n",
      "[ a ] -> [64] \n",
      "[ r ] -> [81] \n",
      "[ r ] -> [81] \n",
      "[ y ] -> [88] \n",
      "[   ] -> [220] \n",
      "[ P ] -> [47] \n",
      "[ o ] -> [78] \n",
      "[ t ] -> [83] \n",
      "[ t ] -> [83] \n",
      "[ e ] -> [68] \n",
      "[ r ] -> [81] \n",
      "[   ] -> [220] \n",
      "[ w ] -> [86] \n",
      "[ a ] -> [64] \n",
      "[ s ] -> [82] \n",
      "[   ] -> [220] \n",
      "[ a ] -> [64] \n",
      "[   ] -> [220] \n",
      "[ w ] -> [86] \n",
      "[ i ] -> [72] \n",
      "[ z ] -> [89] \n",
      "[ a ] -> [64] \n",
      "[ r ] -> [81] \n",
      "[ d ] -> [67] \n",
      "[ . ] -> [13] \n"
     ]
    }
   ],
   "source": [
    "for char in text:\n",
    "    token_ids = tokenizer.encode(char) # 한 글자씩 인코딩\n",
    "    decode =tokenizer.decode(token_ids)\n",
    "    \n",
    "    print (f\"[ {char} ] -> {token_ids} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  한국어 및 한자 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moon/miniconda3/envs/gpu_test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 102400\n",
      "21 15\n",
      "[31980, 1599, 712, 657, 769, 369, 26733, 370, 4605, 4573, 732, 5844, 634, 30556, 375]\n",
      "대사께서는 도(道)를 얻은 모양이구려.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer # pip install transformers\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\")  # KoGPT2 사용\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")  # KoGPT2 사용\n",
    "\n",
    "print(\"Vocab size :\", len(tokenizer))\n",
    "\n",
    "text = \"대사께서는 도(道)를 얻은 모양이구려.\"\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "print(len(text), len(tokens))\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로더(DataLoader)\n",
    "\n",
    "TEXT: Harry Potter was a Wizard. <br/>\n",
    "input(i):  Harry Potter<br/>\n",
    "target(i+1) was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#pytorch Dataset상속받아서 만드는\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt, max_length, stride):\n",
    "        self.input_ids = [] \n",
    "        self.target_ids = []\n",
    "        \n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        \n",
    "        print(\"# of tokens in txt:\", len(token_ids))\n",
    "        \n",
    "        for i in range(0, len(token_ids) - max_length, stride ):\n",
    "            input_chunk = token_ids[i:      i + max_length ]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens in txt: 123176\n"
     ]
    }
   ],
   "source": [
    "# with open(\"cleaned_한글문서.txt\", 'r', encoding='utf-8-sig') as file: # 선택: -sig를 붙여서 BOM 제거\n",
    "with open(\"/home/moon/LLM_Study/data/harry-potter-books/versions/1/02 Harry Potter and the Chamber of Secrets.txt_cleaned\", 'r', encoding='utf-8-sig') as file: # 선택: -sig를 붙여서 BOM 제거\n",
    "    txt = file.read()\n",
    "    \n",
    "    \n",
    "dataset = MyDataset(txt, max_length=32, stride=4)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", Miss Granger.” Lockhart stepped forward eagerly. “My office is nearest, Headmaster — just upstairs — please feel free —” “Thank you,\n",
      "---\n",
      " Miss Granger.” Lockhart stepped forward eagerly. “My office is nearest, Headmaster — just upstairs — please feel free —” “Thank you, G\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "\n",
    "x, y = next(dataiter)\n",
    "\n",
    "print(tokenizer.decode(x[0].tolist()))\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "print(tokenizer.decode(y[0].tolist()))\n",
    "\n",
    "# 한 단어씩 밀려있는 형태!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴럴네트워크 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 정의할 때 사용하는 상수들\n",
    "VOCAB_SIZE =  tokenizer.vocab_size  # 50257 tiktoken # 단어가 몇 개 있나\n",
    "# VOCAB_SIZE = len(toknizer) # AutoTokenizer\n",
    "\n",
    "CONTEXT_LENGTH = 128  # Shortened context length (orig: 1024)\n",
    "EMB_DIM = 768  # Embedding dimension\n",
    "NUM_HEADS = 12  # Number of attention heads\n",
    "NUM_LAYERS = 12  # Number of layers\n",
    "DROP_RATE = 0.1  # Dropout rate\n",
    "QKV_BIAS = False  # Query-key-value bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_out % NUM_HEADS == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.head_dim = d_out // NUM_HEADS\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(DROP_RATE)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(CONTEXT_LENGTH, CONTEXT_LENGTH), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
    "        values = values.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(EMB_DIM, 4 * EMB_DIM),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * EMB_DIM, EMB_DIM),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=EMB_DIM,\n",
    "            d_out=EMB_DIM)\n",
    "    \n",
    "        self.ff = FeedForward()\n",
    "        self.norm1 = LayerNorm(EMB_DIM)\n",
    "        self.norm2 = LayerNorm(EMB_DIM)\n",
    "        self.drop_shortcut = nn.Dropout(DROP_RATE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(VOCAB_SIZE, EMB_DIM)\n",
    "        self.pos_emb = nn.Embedding(CONTEXT_LENGTH, EMB_DIM)\n",
    "        self.drop_emb = nn.Dropout(DROP_RATE)\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock() for _ in range(NUM_LAYERS)])\n",
    "\n",
    "        self.final_norm = LayerNorm(EMB_DIM)\n",
    "        self.out_head = nn.Linear(EMB_DIM, VOCAB_SIZE, bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens seen: 4096\n",
      "Epoch: 1, Loss: 4.7848054001728695\n",
      "Epoch: 2, Loss: 2.513247337937355\n",
      "Epoch: 3, Loss: 0.9165150910615921\n",
      "Epoch: 4, Loss: 0.40819243565201757\n",
      "Tokens seen: 4100096\n",
      "Epoch: 5, Loss: 0.3051235786328713\n",
      "Epoch: 6, Loss: 0.2688799823323886\n",
      "Epoch: 7, Loss: 0.24879281626393399\n",
      "Epoch: 8, Loss: 0.23681814974794785\n",
      "Tokens seen: 8196096\n",
      "Epoch: 9, Loss: 0.23129487211505573\n",
      "Epoch: 10, Loss: 0.22539350235213837\n"
     ]
    }
   ],
   "source": [
    "tokens_seen, global_step = 0, -1\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for input_batch, target_batch in train_loader:\n",
    "        optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "        logits = model(input_batch)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward() # Calculate loss gradients\n",
    "        optimizer.step() # Update model weights using loss gradients\n",
    "        tokens_seen += input_batch.numel()\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % 1000 == 0:\n",
    "            print(f\"Tokens seen: {tokens_seen}\")\n",
    "        # Optional evaluation step\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {avg_loss}\")\n",
    "    torch.save(model.state_dict(), \"model_\" + str(epoch + 1).zfill(3) + \".pth\")\n",
    "\n",
    "# 주의: 여기서는 편의상 모든 데이터를 train에 사용하였습니다. \n",
    "#      ML에서는 일부 데이터를 validation에 사용하는 것이 일반적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(102400, 768)\n",
       "  (pos_emb): Embedding(128, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파일로 저장했던 네트워크의 가중치들 읽어들이기\n",
    "model.load_state_dict(torch.load(\"model_009.pth\", map_location=device, weights_only=True))\n",
    "model.eval() # dropout을 사용하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.09\t 1814\t  used\n",
      "12.70\t 3431\t  always\n",
      "11.20\t 2669\t  still\n",
      "9.76\t 619\t  a\n",
      "9.70\t 4072\t  sent\n",
      "9.11\t 12640\t  caught\n",
      "8.87\t 8817\t  nearest\n",
      "8.82\t 2992\t  free\n",
      "8.68\t 14861\t  suddenly\n",
      "8.59\t 24253\t  warned\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "idx = tokenizer.encode(\"Dobby is\") # 토큰 id의 list\n",
    "idx = torch.tensor(idx).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(idx)\n",
    "\n",
    "logits = logits[:, -1, :]\n",
    "\n",
    "# 가장 확률이 높은 단어 10개 출력\n",
    "top_logits, top_indices = torch.topk(logits, 10) \n",
    "for p, i in zip(top_logits.squeeze(0).tolist(), top_indices.squeeze(0).tolist()):\n",
    "    print(f\"{p:.2f}\\t {i}\\t {tokenizer.decode([i])}\")\n",
    "\n",
    "# 가장 확률이 높은 단어 출력\n",
    "idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "flat = idx_next.squeeze(0) # 배치 차원 제거 torch.Size([1])\n",
    "out = tokenizer.decode(flat.tolist()) # 텐서를 리스트로 바꿔서 디코드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
